import streamlit as st
import requests
import sqlite3
import logging
import random
import datetime
import yaml
import re
import json
from enum import Enum
from pathlib import Path
from PIL import Image
from manga_ocr import MangaOcr
from streamlit_drawable_canvas import st_canvas
import subprocess
import os

# ---- Path Configuration ----
BASE_DIR = Path(__file__).parent.absolute()
PROJECT_ROOT = BASE_DIR.parent
DATA_DIR = PROJECT_ROOT / "data"
DB_PATH = DATA_DIR / "db.sqlite3"
IMAGES_DIR = BASE_DIR / "images"
PROMPTS_PATH = BASE_DIR / "prompts.yaml"
SENTENCES_PATH = BASE_DIR / "sentences.json"

# ---- Database Schema ----
REQUIRED_TABLES = {
    'word_groups': ['id', 'name', 'level'],
    'words': ['id', 'kanji', 'romaji', 'english', 'group_id'],
    'sentences': ['id', 'japanese', 'english', 'level', 'category'],
    'writing_submissions': ['id', 'sentence_id', 'transcription', 'translation', 'grade', 'feedback']
}

# ---- Logging Configuration ----
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(BASE_DIR / "app.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ---- API Health Check ----
def check_api_health():
    """Check if JSON API server is running"""
    try:
        response = requests.get("http://localhost:5000/api/health", timeout=2)
        return response.status_code == 200
    except (requests.ConnectionError, requests.Timeout):
        return False

# ---- LLM Health Check ----
def check_llm_health():
    """Check if LLM server is running"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        return response.status_code == 200
    except (requests.ConnectionError, requests.Timeout):
        return False

# ---- Load Prompts ----
def load_prompts():
    """Load generation prompts from YAML file"""
    try:
        with open(PROMPTS_PATH, "r") as f:
            return yaml.safe_load(f)
    except Exception as e:
        logger.error(f"Error loading prompts: {e}")
        return {}

# ---- Load Sample Sentences ----
def load_sample_sentences():
    """Load sample sentences from JSON file"""
    try:
        with open(SENTENCES_PATH, "r") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Error loading sample sentences: {e}")
        return []

# ---- Application States ----
class AppState(Enum):
    SETUP = "setup"
    PRACTICE = "practice"
    REVIEW = "review"

class WritingPracticeApp:
    def __init__(self):
        self.initialize_session_state()
        self.load_vocabulary_groups()
        self.prompts = load_prompts()
        self.sample_sentences = load_sample_sentences()
        self.mocr = None
        self.error_log = []

    def initialize_session_state(self):
        defaults = {
            'app_state': AppState.SETUP,
            'current_sentence': None,
            'review_data': None,
            'current_group_id': None,
            'vocabulary': []
        }
        for key, val in defaults.items():
            if key not in st.session_state:
                st.session_state[key] = val

    def validate_database(self):
        try:
            with sqlite3.connect(DB_PATH) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                existing_tables = {row[0] for row in cursor.fetchall()}
                missing_tables = set(REQUIRED_TABLES.keys()) - existing_tables
                if missing_tables:
                    raise ValueError(f"Missing tables: {', '.join(missing_tables)}")
        except Exception as e:
            logger.error(f"Database validation failed: {e}")
            st.error("Database configuration error")
            st.stop()

    def set_background(self, image_name: str):
        try:
            bg_path = IMAGES_DIR / image_name
            st.markdown(
                f"""
                <style>
                .stApp {{
                    background: url('{bg_path}') no-repeat center center fixed;
                    background-size: cover;
                }}
                .main .block-container {{
                    background-color: rgba(255, 255, 255, 0.95);
                    border-radius: 10px;
                    padding: 2rem;
                    margin: 2rem auto;
                    max-width: 90%;
                }}
                </style>
                """,
                unsafe_allow_html=True
            )
        except Exception as e:
            logger.error(f"Background error: {e}")
            st.markdown("""<style>.stApp { background-color: #f0f2f6; }</style>""", unsafe_allow_html=True)

    def render_setup_state(self):
        self.set_background("1240417.png")
        st.title("Japanese Writing Practice")

        # API and LLM status indicators
        api_status = "🟢 Running" if check_api_health() else "🔴 Offline"
        llm_status = "🟢 Running" if check_llm_health() else "🔴 Offline"
        st.markdown(f"**API Status:** {api_status}")
        st.markdown(f"**LLM Status:** {llm_status}")

        if not self.vocabulary_groups:
            st.error("No vocabulary categories found")
            return

        if not check_api_health():
            st.warning("""
            **Connection Issue Detected**
            Trying fallback database access. Some features might be limited.
            - Ensure the API server is running in another terminal:
            `python api_server.py`
            - Check firewall settings if running remotely
            """)

        # Category selection dropdown
        selected_group_id = st.selectbox(
            "Select a Category",
            options=[g['id'] for g in self.vocabulary_groups],
            format_func=lambda x: next(g['name'] for g in self.vocabulary_groups if g['id'] == x)
        )

        if st.button(
            "Generate Practice Sentence 🎯",
            type="primary",
            use_container_width=True,
            key="generate-btn"
        ):
            try:
                logger.debug(f"Attempting to load vocabulary for group_id: {selected_group_id}")
                self.load_vocabulary(selected_group_id)

                with st.spinner("Generating sentence..."):
                    logger.debug("Generating sentence...")
                    sentence = self.generate_sentence_safe()

                    if sentence:
                        logger.debug(f"Generated sentence: {sentence}")
                        self.cache_sentence(sentence)
                        st.session_state.update({
                            'current_sentence': sentence,
                            'app_state': AppState.PRACTICE,
                            'current_group_id': selected_group_id
                        })
                        logger.debug(f"Updated session state: {st.session_state}")
                        st.rerun()
                    else:
                        st.error("Failed to generate sentence")
                        logger.error("Failed to generate sentence")
            except Exception as e:
                logger.error(f"Generation failed: {e}")
                st.error(f"Error generating practice session: {str(e)}")

        # Display current category if available
        if st.session_state.get('current_group_id'):
            try:
                group = next(g for g in self.vocabulary_groups if g['id'] == st.session_state.current_group_id)
                st.markdown(f"*Last practiced: {group['name']} (Level {group['level']})*")
            except StopIteration:
                pass

    def render_practice_state(self):
        self.set_background("1371442.png")
        st.title("Practice Writing")

        if not st.session_state.current_sentence:
            logger.error("No current sentence found in session state")
            st.session_state.app_state = AppState.SETUP
            st.rerun()
            return

        col1, col2 = st.columns([3, 2])
        with col1:
            st.subheader("Current Sentence")
            st.markdown(f"""
            **Japanese:**
            {st.session_state.current_sentence['japanese']}
            **English:**
            {st.session_state.current_sentence['english']}
            """)

        with col2:
            st.subheader("Write Your Response")
            canvas_result = st_canvas(
                fill_color="rgba(255, 255, 255, 0.0)",
                stroke_width=2,
                stroke_color='#000',
                background_color="#ffff",
                height=300,
                width=400,
                drawing_mode="freedraw",
                key="canvas"
            )

            if st.button("Submit for Review"):
                if canvas_result.image_data is not None:
                    img = Image.fromarray(canvas_result.image_data.astype('uint8'))
                    recognized_text = self.recognize_text(img)
                    self.process_submission(recognized_text)

            if st.button("Back to Main Menu"):
                st.session_state.app_state = AppState.SETUP
                st.rerun()

    def render_review_state(self):
        self.set_background("1371443.png")
        st.title("Submission Review")

        review = st.session_state.review_data
        grade_color = {
            'S': '#4CAF50',
            'A': '#8BC34A',
            'B': '#FFC107',
            'C': '#F44336'
        }.get(review['grade'], '#607D8B')

        st.markdown(f"""
        <div style="
        background-color: {grade_color};
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
        margin: 1rem 0;
        ">
        <h2>Grade: {review['grade']}</h2>
        <p>{review['feedback']}</p>
        </div>
        """, unsafe_allow_html=True)

        col1, col2 = st.columns(2)
        with col1:
            st.subheader("Your Submission")
            st.text_area("", value=review['submitted_text'], disabled=True)

        with col2:
            st.subheader("Correct Answer")
            st.text_area("", value=review['correct_text'], disabled=True)

        if st.button("Try Again"):
            st.session_state.app_state = AppState.PRACTICE
            st.rerun()

        if st.button("New Sentence"):
            st.session_state.app_state = AppState.SETUP
            st.rerun()

    def load_vocabulary_groups(self):
        try:
            with sqlite3.connect(DB_PATH) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()
                cursor.execute("SELECT id, name, level FROM word_groups ORDER BY level")
                self.vocabulary_groups = [dict(row) for row in cursor.fetchall()]
                logger.info(f"Loaded {len(self.vocabulary_groups)} vocabulary groups")
        except Exception as e:
            logger.error(f"Error loading groups: {e}")
            self.vocabulary_groups = []
            st.error("Failed to load vocabulary categories")

    def load_vocabulary(self, group_id: int):
        """Load vocabulary for selected group with enhanced error handling"""
        try:
            if not check_api_health():
                raise RuntimeError("API server not responding")

            response = requests.get(
                f"http://localhost:5000/api/groups/{group_id}/raw",
                timeout=3
            )
            response.raise_for_status()
            st.session_state.vocabulary = response.json()
            logger.info(f"Loaded {len(st.session_state.vocabulary)} words via API")
            return

        except Exception as api_error:
            logger.warning(f"API load failed: {api_error}, falling back to direct DB access")
            try:
                with sqlite3.connect(DB_PATH) as conn:
                    conn.row_factory = sqlite3.Row
                    cursor = conn.cursor()
                    cursor.execute("""
                    SELECT id, kanji, romaji, english
                    FROM words
                    WHERE group_id = ?
                    """, (group_id,))
                    st.session_state.vocabulary = [dict(row) for row in cursor.fetchall()]
                    logger.info(f"Loaded {len(st.session_state.vocabulary)} words from DB")

                if not st.session_state.vocabulary:
                    raise ValueError("No words found in database for selected group")

            except Exception as db_error:
                logger.error(f"Vocabulary load failed: {db_error}")
                st.error("Failed to load vocabulary from both API and database")
                self.error_log.append({
                    'timestamp': datetime.datetime.now(),
                    'group_id': group_id,
                    'error': str(db_error)
                })
                st.session_state.app_state = AppState.SETUP
                st.rerun()

    def generate_sentence_safe(self):
        """Generate a sentence with multiple fallback options"""
        # First try using Ollama
        try:
            if check_llm_health():
                sentence = self.generate_sentence_with_ollama()
                if sentence:
                    return sentence
                logger.warning("Ollama generation failed, trying curl fallback")
            else:
                logger.warning("Ollama not available, trying curl fallback")
        except Exception as e:
            logger.error(f"Ollama error: {e}")

        # Try using curl as a fallback
        try:
            sentence = self.generate_sentence_with_curl()
            if sentence:
                return sentence
            logger.warning("Curl generation failed, using sample sentence")
        except Exception as e:
            logger.error(f"Curl error: {e}")

        # Use sample sentences as final fallback
        return self.get_sample_sentence()

    def generate_sentence_with_ollama(self):
        """Generate sentence using Ollama Python client"""
        if not st.session_state.vocabulary:
            logger.error("Vocabulary list is empty. Cannot generate sentence.")
            return None

        try:
            import ollama
            selected_word = random.choice(st.session_state.vocabulary)
            prompt = self.prompts['sentence_generation']
            system_prompt = prompt['system']
            user_prompt = prompt['user'].format(word=selected_word['kanji'])

            logger.debug(f"Sending prompt to Ollama: {user_prompt}")
            response = ollama.generate(model='llama3.2', prompt=f"{system_prompt}\n\nUser: {user_prompt}")
            logger.debug(f"Ollama response: {response}")

            # Try to parse the response
            japanese, english = self.parse_llm_response(response)

            if japanese and english:
                group = next(g for g in self.vocabulary_groups if g['id'] == st.session_state.current_group_id)
                return {
                    "japanese": japanese,
                    "english": english,
                    "level": group['level'],
                    "category": group['name']
                }

            return None
        except Exception as e:
            logger.error(f"Ollama generation failed: {e}")
            return None

    def generate_sentence_with_curl(self):
        """Generate sentence using curl command as fallback"""
        if not st.session_state.vocabulary:
            logger.error("Vocabulary list is empty. Cannot generate sentence.")
            return None

        try:
            selected_word = random.choice(st.session_state.vocabulary)
            prompt = self.prompts['sentence_generation']
            system_prompt = prompt['system']
            user_prompt = prompt['user'].format(word=selected_word['kanji'])

            # Prepare the request payload
            payload = {
                "model": "llama3.2",
                "prompt": f"{system_prompt}\n\nUser: {user_prompt}",
                "stream": False
            }

            # Save payload to a temporary file
            with open("temp_payload.json", "w") as f:
                json.dump(payload, f)

            # Execute curl command
            cmd = "curl -s http://localhost:11434/api/generate -d @temp_payload.json"
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)

            # Clean up temp file
            if os.path.exists("temp_payload.json"):
                os.remove("temp_payload.json")

            if result.returncode != 0:
                logger.error(f"Curl command failed: {result.stderr}")
                return None

            # Parse the response
            response_data = json.loads(result.stdout)
            response_text = response_data.get("response", "")

            # Try to parse the response
            japanese, english = self.parse_llm_response(response_text)

            if japanese and english:
                group = next(g for g in self.vocabulary_groups if g['id'] == st.session_state.current_group_id)
                return {
                    "japanese": japanese,
                    "english": english,
                    "level": group['level'],
                    "category": group['name']
                }

            return None
        except Exception as e:
            logger.error(f"Curl generation failed: {e}")
            return None

    def get_sample_sentence(self):
        """Get a sample sentence as fallback"""
        if not self.sample_sentences:
            # Create a default sentence if no samples available
            return {
                "japanese": "これは日本語の文です。",
                "english": "This is a Japanese sentence.",
                "level": "N5",
                "category": "Fallback"
            }

        # Get a random sample sentence
        sample = random.choice(self.sample_sentences)

        # Add current category if available
        if st.session_state.get('current_group_id'):
            try:
                group = next(g for g in self.vocabulary_groups if g['id'] == st.session_state.current_group_id)
                sample["category"] = group['name']
                sample["level"] = group['level']
            except StopIteration:
                pass

        return sample

    def parse_llm_response(self, response_text):
        """Parse the LLM response to extract Japanese and English sentences"""
        try:
            logger.debug(f"Parsing response: {response_text}")

            # Try the original approach first
            if 'Japanese:' in response_text and 'English:' in response_text:
                japanese = response_text.split('Japanese:')[1].split('English:')[0].strip()
                english = response_text.split('English:')[1].strip()
                logger.debug(f"Parsed with format markers: JP={japanese}, EN={english}")
                return japanese, english

            # Alternative parsing if the format is different
            # Look for Japanese text (usually contains kanji/hiragana)
            japanese_pattern = r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]+'
            japanese_matches = re.findall(japanese_pattern, response_text)

            if japanese_matches:
                # Join all Japanese text segments
                japanese_text = ''.join(japanese_matches).strip()

                # For English, take text after the Japanese text
                # This is a simplification - in real code you'd need more robust parsing
                english_parts = re.split(japanese_pattern, response_text)
                english_text = ' '.join([part.strip() for part in english_parts if part.strip()]).strip()

                # Clean up the texts
                japanese_text = japanese_text.replace(' ', '')  # Remove spaces in Japanese text

                logger.debug(f"Parsed with regex: JP={japanese_text}, EN={english_text}")

                # If we have both Japanese and English text, return them
                if japanese_text and english_text:
                    return japanese_text, english_text

            # If we couldn't parse the response, log it and return None
            logger.error(f"Could not parse LLM response: {response_text}")
            return None, None
        except Exception as e:
            logger.error(f"Error parsing LLM response: {e}")
            return None, None

    def cache_sentence(self, sentence_data):
        try:
            with sqlite3.connect(DB_PATH) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                INSERT INTO sentences (japanese, english, level, category)
                VALUES (?, ?, ?, ?)
                """, (
                    sentence_data['japanese'],
                    sentence_data['english'],
                    sentence_data['level'],
                    sentence_data['category']
                ))
                conn.commit()
            logger.info("Cached generated sentence")
        except Exception as e:
            logger.error(f"Failed to cache sentence: {e}")

    def recognize_text(self, image):
        if self.mocr is None:
            try:
                self.mocr = MangaOcr()
            except Exception as e:
                logger.error(f"Failed to initialize MangaOcr: {e}")
                return "OCR initialization failed"

        try:
            return self.mocr(image)
        except Exception as e:
            logger.error(f"OCR recognition failed: {e}")
            return "OCR recognition failed"

    def process_submission(self, recognized_text):
        correct_text = st.session_state.current_sentence['japanese']
        grade = 'S' if recognized_text.strip() == correct_text.strip() else 'C'

        st.session_state.review_data = {
            'grade': grade,
            'submitted_text': recognized_text,
            'correct_text': correct_text,
            'feedback': self.generate_feedback(grade)
        }
        st.session_state.app_state = AppState.REVIEW
        st.rerun()

    def generate_feedback(self, grade):
        return {
            'S': "Perfect! Your writing matches exactly!",
            'A': "Very close! Minor improvements needed.",
            'B': "Good attempt. Practice stroke order.",
            'C': "Needs work. Compare with correct answer."
        }.get(grade, "Unknown grade")

    def run(self):
        try:
            self.validate_database()

            if st.session_state.app_state == AppState.SETUP:
                self.render_setup_state()
            elif st.session_state.app_state == AppState.PRACTICE:
                self.render_practice_state()
            elif st.session_state.app_state == AppState.REVIEW:
                self.render_review_state()
        except Exception as e:
            logger.error(f"Application error: {e}")
            st.error(f"A critical error occurred. Please refresh the page. Error: {str(e)}")

if __name__ == "__main__":
    WritingPracticeApp().run()