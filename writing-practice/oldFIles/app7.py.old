import streamlit as st
import sqlite3
import logging
import random
import json
from pathlib import Path
import requests
import io
from PIL import Image
import base64
import numpy as np
import cv2
import sys
import subprocess
import tempfile

# ---- Path Configuration ----
BASE_DIR = Path(__file__).parent.absolute()
PROJECT_ROOT = BASE_DIR.parent
DATA_DIR = PROJECT_ROOT / "data"
DB_PATH = DATA_DIR / "db.sqlite3"
SENTENCES_PATH = BASE_DIR / "sentences.json"

# ---- Logging Configuration ----
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(BASE_DIR / "app.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Check if MangaOCR is installed, if not, install it
try:
    import manga_ocr
except ImportError:
    logger.info("Installing manga_ocr...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "manga-ocr"])
    import manga_ocr

# Initialize MangaOCR
@st.cache_resource
def get_ocr():
    try:
        return manga_ocr.MangaOcr()
    except Exception as e:
        logger.error(f"Error initializing MangaOCR: {e}")
        return None

# Load sample sentences
def load_sample_sentences():
    try:
        with open(SENTENCES_PATH, "r") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Error loading sample sentences: {e}")
        return []

# Process image with MangaOCR
def process_image_with_ocr(image):
    ocr = get_ocr()
    if ocr is None:
        return "OCR not available"

    try:
        # Convert to numpy array if it's not already
        if isinstance(image, Image.Image):
            img_array = np.array(image)
        else:
            img_array = np.array(Image.open(image))

        # MangaOCR expects RGB images
        if len(img_array.shape) == 2:  # Grayscale
            img_array = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)
        elif img_array.shape[2] == 4:  # RGBA
            img_array = cv2.cvtColor(img_array, cv2.COLOR_RGBA2RGB)

        # Save to a temporary file (MangaOCR works better with files)
        with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
            Image.fromarray(img_array).save(tmp.name)
            text = ocr(tmp.name)

        return text
    except Exception as e:
        logger.error(f"OCR error: {e}")
        return "Error processing image"

# Grade user response
def grade_response(original, user_input):
    prompt = f"""
    Grade this Japanese writing practice:

    Original sentence: {original}
    User's response: {user_input}

    Provide feedback on grammar, vocabulary, and overall accuracy.
    Rate on a scale of 1-10.
    Format your response as:

    Score: [1-10]
    Feedback: [Your detailed feedback]
    """

    try:
        # Try using Ollama API
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "llama3",
                "prompt": prompt,
                "stream": False
            },
            timeout=10
        )

        if response.status_code == 200:
            result = response.json()
            return result.get("response", "Error processing response")
    except Exception as e:
        logger.warning(f"Ollama API error: {e}")

    # Fallback grading
    if user_input.strip() == "":
        return "Score: 0\nFeedback: No response provided."
    elif user_input.strip() == original.strip():
        return "Score: 10\nFeedback: Perfect match with the original sentence!"
    else:
        # Simple character-based similarity
        similarity = len(set(user_input) & set(original)) / len(set(original))
        score = round(similarity * 10)
        return f"Score: {score}\nFeedback: Your response has approximately {int(similarity*100)}% character overlap with the original."

# Create a drawing canvas
def create_drawing_canvas():
    canvas_result = st.empty()

    # Create a canvas component
    canvas_html = """
    <canvas id="canvas" width="600" height="300" style="border:1px solid #000000;"></canvas>
    <br>
    <button id="clear-button">Clear Canvas</button>
    <script>
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        let isDrawing = false;

        // Set up drawing properties
        ctx.lineWidth = 3;
        ctx.lineCap = 'round';
        ctx.strokeStyle = 'black';

        // Drawing event listeners
        canvas.addEventListener('mousedown', startDrawing);
        canvas.addEventListener('touchstart', startDrawingTouch);
        canvas.addEventListener('mousemove', draw);
        canvas.addEventListener('touchmove', drawTouch);
        canvas.addEventListener('mouseup', stopDrawing);
        canvas.addEventListener('touchend', stopDrawing);
        canvas.addEventListener('mouseout', stopDrawing);

        // Clear button
        document.getElementById('clear-button').addEventListener('click', clearCanvas);

        function startDrawing(e) {
            isDrawing = true;
            draw(e);
        }

        function startDrawingTouch(e) {
            isDrawing = true;
            drawTouch(e);
        }

        function draw(e) {
            if (!isDrawing) return;

            ctx.beginPath();
            ctx.moveTo(e.offsetX, e.offsetY);
            ctx.lineTo(e.offsetX, e.offsetY);
            ctx.stroke();
        }

        function drawTouch(e) {
            if (!isDrawing) return;
            e.preventDefault();

            const rect = canvas.getBoundingClientRect();
            const touch = e.touches[0];
            const x = touch.clientX - rect.left;
            const y = touch.clientY - rect.top;

            ctx.beginPath();
            ctx.moveTo(x, y);
            ctx.lineTo(x, y);
            ctx.stroke();
        }

        function stopDrawing() {
            isDrawing = false;

            // Send the image data to Streamlit
            const imageData = canvas.toDataURL('image/png');
            window.parent.postMessage({
                type: 'canvas-data',
                data: imageData
            }, '*');
        }

        function clearCanvas() {
            ctx.clearRect(0, 0, canvas.width, canvas.height);

            // Send empty data to Streamlit
            window.parent.postMessage({
                type: 'canvas-data',
                data: null
            }, '*');
        }
    </script>
    """

    canvas_result.html(canvas_html, height=350)

    # Create a placeholder for the image data
    if 'canvas_image' not in st.session_state:
        st.session_state.canvas_image = None

    # JavaScript callback handler
    components.html(
        """
        <script>
            window.addEventListener('message', function(event) {
                if (event.data.type === 'canvas-data') {
                    window.parent.postMessage({
                        type: 'streamlit:setComponentValue',
                        value: event.data.data
                    }, '*');
                }
            });
        </script>
        """,
        height=0,
    )

    return canvas_result

# Initialize session state
if 'current_sentence' not in st.session_state:
    st.session_state.current_sentence = None

if 'vocabulary_groups' not in st.session_state:
    try:
        with sqlite3.connect(DB_PATH) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute("SELECT id, name, level FROM word_groups ORDER BY level")
            st.session_state.vocabulary_groups = [dict(row) for row in cursor.fetchall()]
            logger.info(f"Loaded {len(st.session_state.vocabulary_groups)} vocabulary groups")
    except Exception as e:
        logger.error(f"Error loading groups: {e}")
        st.session_state.vocabulary_groups = []

# Import streamlit_drawable_canvas
try:
    from streamlit_drawable_canvas import st_canvas
except ImportError:
    logger.info("Installing streamlit_drawable_canvas...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "streamlit-drawable-canvas"])
    from streamlit_drawable_canvas import st_canvas

# Main app
st.title("Japanese Writing Practice")

# Display vocabulary groups
if not st.session_state.vocabulary_groups:
    st.error("No vocabulary categories found")
else:
    # Category selection dropdown
    selected_group_id = st.selectbox(
        "Select a Category",
        options=[g['id'] for g in st.session_state.vocabulary_groups],
        format_func=lambda x: next(g['name'] for g in st.session_state.vocabulary_groups if g['id'] == x)
    )

    # Generate button
    if st.button("Generate Practice Sentence ðŸŽ¯", type="primary"):
        # Use a sample sentence
        sample_sentences = load_sample_sentences()
        if sample_sentences:
            st.session_state.current_sentence = random.choice(sample_sentences)
        else:
            # Fallback sentence
            st.session_state.current_sentence = {
                "japanese": "ã“ã‚Œã¯æ—¥æœ¬èªžã®æ–‡ã§ã™ã€‚",
                "english": "This is a Japanese sentence.",
                "level": "N5",
                "category": "Fallback"
            }

# Display current sentence if available
if st.session_state.current_sentence:
    st.subheader("Current Sentence")
    st.markdown(f"""
    **Japanese:**
    {st.session_state.current_sentence['japanese']}
    **English:**
    {st.session_state.current_sentence['english']}
    """)

    # Add a writing area
    st.subheader("Submit Your Response")

    # Create tabs for different input methods
    tab1, tab2, tab3 = st.tabs(["Draw Response", "Upload Image", "Take Photo"])

    with tab1:
        st.write("Draw your response below:")

        # Create a canvas for drawing
        canvas_result = st_canvas(
            fill_color="rgba(255, 255, 255, 0)",
            stroke_width=3,
            stroke_color="#000000",
            background_color="#ffffff",
            height=300,
            width=600,
            drawing_mode="freedraw",
            key="canvas",
        )

        if st.button("Submit Drawing"):
            if canvas_result.image_data is not None:
                # Convert the canvas image to PIL Image
                img_array = canvas_result.image_data
                if img_array.sum() > 0:  # Check if the canvas has any drawing
                    img = Image.fromarray(img_array.astype('uint8'), 'RGBA')

                    # Display the image
                    st.image(img, caption="Your Drawing", use_column_width=True)

                    # Process with OCR
                    recognized_text = process_image_with_ocr(img)
                    st.write(f"Recognized text: {recognized_text}")

                    # Grade the response
                    feedback = grade_response(st.session_state.current_sentence['japanese'], recognized_text)

                    # Display feedback
                    st.subheader("Feedback")
                    st.markdown(feedback)
                else:
                    st.warning("Please draw something before submitting.")
            else:
                st.warning("Please draw something before submitting.")

    with tab2:
        uploaded_file = st.file_uploader("Upload an image of your handwritten response", type=["jpg", "jpeg", "png"])
        if uploaded_file is not None:
            # Display the uploaded image
            image = Image.open(uploaded_file)
            st.image(image, caption="Uploaded Image", use_column_width=True)

            if st.button("Submit Uploaded Image"):
                # Process with OCR
                recognized_text = process_image_with_ocr(uploaded_file)
                st.write(f"Recognized text: {recognized_text}")

                # Grade the response
                feedback = grade_response(st.session_state.current_sentence['japanese'], recognized_text)

                # Display feedback
                st.subheader("Feedback")
                st.markdown(feedback)

    with tab3:
        st.write("Take a photo of your handwritten response")
        picture = st.camera_input("Take a picture")

        if picture:
            # Display the captured image
            st.image(picture, caption="Captured Image", use_column_width=True)

            if st.button("Submit Photo"):
                # Process with OCR
                recognized_text = process_image_with_ocr(picture)
                st.write(f"Recognized text: {recognized_text}")

                # Grade the response
                feedback = grade_response(st.session_state.current_sentence['japanese'], recognized_text)

                # Display feedback
                st.subheader("Feedback")
                st.markdown(feedback)