# Main docker compose file

services:
  ollama-server:
    image: ollama/ollama
    container_name: ollama-server
    ports:
      - "${LLM_ENDPOINT_PORT:-8008}:11434"
    volumes:
      - ${OLLAMA_DATA_PATH:-./data/ollama_data}:/root/.ollama
      - ${SHARED_DB_PATH:-./data/shared_db}:/app/db
    restart: unless-stopped

  llm_text:
    build:
      context: ./comps/llm_text
    container_name: llm_text
    ports:
      - "${LLM_TEXT_PORT:-9000}:9000"
    environment:
      - LLM_ENDPOINT=${LLM_ENDPOINT}
      - LLM_MODEL_ID=${LLM_MODEL_ID}
    volumes:
      - ./comps/llm_text:/app
      - ${SHARED_DB_PATH:-./data/shared_db}:/app/db
      - ${LLM_XTTS_DATA_PATH:-./data/llm_xtts}:/app/data/xtts
    depends_on:
      - ollama-server
    restart: unless-stopped

  guardrails:
    build:
      context: ./comps/guardrails
    container_name: guardrails
    ports:
      - "${GUARDRAILS_SERVICE_PORT:-9400}:9400"
    volumes:
      - ./comps/guardrails:/app
      - ${SHARED_DB_PATH:-./data/shared_db}:/app/db
    restart: unless-stopped

  chromadb:
    image: ghcr.io/chroma-core/chroma:latest
    container_name: chromadb
    ports:
      - "${CHROMADB_PORT:-8050}:8000"
    volumes:
      - ${CHROMA_DATA_PATH:-./data/chroma_data}:/chroma/chroma
    restart: unless-stopped

  tts:
    build:
      context: ./comps/tts
    container_name: tts
    ports:
      - "${TTS_SERVICE_PORT:-9200}:9200"
    environment:
      - TTS_MODEL=${TTS_MODEL}
    volumes:
      - ./comps/tts:/app
      - ${TTS_DATA_PATH:-./data/tts_data}:/app/data
    restart: unless-stopped

  asr:
    build:
      context: ./comps/asr
    container_name: asr
    ports:
      - "${ASR_SERVICE_PORT:-9300}:9300"
    environment:
      - ASR_MODEL=${ASR_MODEL}
    volumes:
      - ./comps/asr:/app
      - ${ASR_DATA_PATH:-./data/asr_data}:/app/data
    restart: unless-stopped

  llm-vision:
    build:
      context: ./comps/llm-vision
    container_name: llm-vision
    ports:
      - "${LLM_VISION_PORT:-9100}:9100"
    environment:
      - VISION_MODEL_ID=${VISION_MODEL_ID}
    volumes:
      - ./comps/llm-vision:/app
      - ${MANGAOCR_MODELS_PATH:-./data/mangaocr_models}:/app/models
    depends_on:
      - ollama-server
    restart: unless-stopped

  waifu-diffusion:
    build:
      context: ./comps/waifu-diffusion
    container_name: waifu-diffusion
    ports:
      - "${WAIFU_DIFFUSION_PORT:-9500}:9500"
    environment:
      - MODEL_ID=${WAIFU_MODEL_ID:-waifu-diffusion/wd-1-5-beta2}
      - MODEL_PATH=/app/data/waifu
      - USE_LOCAL=True
    volumes:
      - ./comps/waifu-diffusion:/app
      - ${WAIFU_DATA_PATH:-./data/waifu}:/app/data/waifu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  embeddings:
    build:
      context: ./comps/embeddings
    container_name: embeddings
    ports:
      - "${EMBEDDING_SERVICE_PORT:-6000}:6000"
    environment:
      - EMBEDDING_MODEL=${EMBEDDING_MODEL}
    volumes:
      - ./comps/embeddings:/app
      - ${SHARED_DB_PATH:-./data/shared_db}:/app/db
    restart: unless-stopped