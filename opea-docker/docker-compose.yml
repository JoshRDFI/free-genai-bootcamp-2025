# Main docker compose file

services:
  ollama-server:
    image: ollama/ollama:latest
    container_name: ollama-server
    runtime: nvidia
    ports:
      - "${LLM_ENDPOINT_PORT:-8008}:11434"
    volumes:
      - ${OLLAMA_DATA_PATH:-../data/ollama_data}:/root/.ollama
      - ${SHARED_DB_PATH:-../data/shared_db}:/app/db
    environment:
      - LLM_MODEL_ID=${LLM_MODEL_ID:-llama3.2}
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  llm_text:
    image: opea-docker-llm-text:latest
    container_name: llm_text
    runtime: nvidia
    ports:
      - "${LLM_TEXT_PORT:-9000}:9000"
    volumes:
      - ${OLLAMA_DATA_PATH:-../data/ollama_data}:/root/.ollama
      - ${SHARED_DB_PATH:-../data/shared_db}:/app/db
      - ${LLM_XTTS_DATA_PATH:-../data/llm_xtts}:/home/llm/.xtts_data
    environment:
      - OLLAMA_HOST=ollama-server
      - LLM_SERVICE_PORT=${LLM_TEXT_PORT:-9000}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-llama3.2}
    depends_on:
      - ollama-server
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  guardrails:
    image: opea-docker-guardrails:latest
    container_name: guardrails
    runtime: nvidia
    ports:
      - "${GUARDRAILS_SERVICE_PORT:-9400}:9400"
    volumes:
      - ${OLLAMA_DATA_PATH:-../data/ollama_data}:/root/.ollama
      - ${SHARED_DB_PATH:-../data/shared_db}:/app/db
    environment:
      - OLLAMA_HOST=ollama-server
      - GUARDRAILS_SERVICE_PORT=${GUARDRAILS_SERVICE_PORT:-9400}
    depends_on:
      - ollama-server
      - embeddings
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  chromadb:
    image: opea-docker-chromadb:latest
    container_name: chromadb
    ports:
      - "${CHROMADB_PORT:-8000}:8000"
    volumes:
      - ${CHROMA_DATA_PATH:-../data/chroma_data}:/chroma/chroma
    environment:
      - ALLOW_RESET=true
      - ANONYMIZED_TELEMETRY=false
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_HTTP_PORT=8000
      - CHROMA_SERVER_CORS_ALLOW_ORIGINS=["*"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  tts:
    image: opea-docker-tts:latest
    container_name: tts
    runtime: nvidia
    ports:
      - "${TTS_SERVICE_PORT:-9200}:9200"
    volumes:
      - ${TTS_DATA_PATH:-../data/tts_data}:/app/data/tts_data
      - ${SHARED_DB_PATH:-../data/shared_db}:/app/db
    environment:
      - TTS_HOME=/app/data/tts_data
      - TTS_DATA_PATH=/app/data/tts_data
      - TTS_MODEL=${TTS_MODEL:-xtts_v2}
      - TTS_SERVICE_PORT=${TTS_SERVICE_PORT:-9200}
    depends_on:
      - llm_text
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  asr:
    image: opea-docker-asr:latest
    container_name: asr
    runtime: nvidia
    ports:
      - "${ASR_SERVICE_PORT:-9300}:9300"
    volumes:
      - ${ASR_DATA_PATH:-../data/asr_data}:/app/data/whisper
      - ${SHARED_DB_PATH:-../data/shared_db}:/app/db
    environment:
      - ASR_SERVICE_PORT=${ASR_SERVICE_PORT:-9300}
      - ASR_MODEL=${ASR_MODEL:-large-v3}
    depends_on:
      - tts
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  llm-vision:
    image: opea-docker-llm-vision:latest
    container_name: llm-vision
    runtime: ${DOCKER_RUNTIME:-nvidia}
    ports:
      - "${LLM_VISION_PORT:-9101}:9101"
    volumes:
      - ${SHARED_DB_PATH:-../data/shared_db}:/app/db
    environment:
      - VISION_SERVICE_PORT=${LLM_VISION_PORT:-9101}
      - VISION_MODEL_ID=${VISION_MODEL_ID:-llava-hf/llava-1.5-7b-hf}
    depends_on:
      - llm_text
    deploy:
      resources:
        reservations:
          devices:
            - driver: ${GPU_DRIVER:-nvidia}
              count: ${GPU_COUNT:-all}
              capabilities: [gpu]
    restart: unless-stopped

  mangaocr:
    image: opea-docker-mangaocr:latest
    container_name: mangaocr
    runtime: ${DOCKER_RUNTIME:-nvidia}
    ports:
      - "${MANGAOCR_PORT:-9100}:9100"
    volumes:
      - ${MANGAOCR_MODELS_PATH:-../data/mangaocr_models}:/app/data/mangaocr_models
      - ${SHARED_DB_PATH:-../data/shared_db}:/app/db
    environment:
      - MANGAOCR_MODEL_PATH=/app/data/mangaocr_models
      - MANGAOCR_SERVICE_PORT=${MANGAOCR_PORT:-9100}
    depends_on:
      - llm_text
    deploy:
      resources:
        reservations:
          devices:
            - driver: ${GPU_DRIVER:-nvidia}
              count: ${GPU_COUNT:-all}
              capabilities: [gpu]
    restart: unless-stopped

  waifu-diffusion:
    image: opea-docker-waifu-diffusion:latest
    container_name: waifu-diffusion
    runtime: nvidia
    ports:
      - "${WAIFU_DIFFUSION_PORT:-9500}:9500"
    volumes:
      - ${WAIFU_DATA_PATH:-../data/waifu}:/app/data/waifu-diffusion
      - ${SHARED_DB_PATH:-../data/shared_db}:/app/db
    environment:
      - WAIFU_DIFFUSION_PORT=${WAIFU_DIFFUSION_PORT:-9500}
      - WAIFU_MODEL_ID=${WAIFU_MODEL_ID:-hakurei/waifu-diffusion-v1-4}
    depends_on:
      - llm_text
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  embeddings:
    image: opea-docker-embeddings:latest
    container_name: embeddings
    ports:
      - "${EMBEDDING_SERVICE_PORT:-6000}:6000"
    volumes:
      - ${EMBEDDING_DATA_PATH:-../data/embeddings}:/app/data/embeddings
      - ${SHARED_DB_PATH:-../data/shared_db}:/app/db
    environment:
      - EMBEDDING_SERVICE_HOST_IP=0.0.0.0
      - EMBEDDING_SERVICE_PORT=${EMBEDDING_SERVICE_PORT:-6000}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-all-MiniLM-L6-v2}
    depends_on:
      - chromadb
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

networks:
  default:
    name: opea-docker_default
