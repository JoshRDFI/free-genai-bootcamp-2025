# Main docker compose file

services:
  ollama-server:
    image: ollama/ollama:latest
    container_name: ollama-server
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ${OLLAMA_DATA_PATH:-/data/ollama_data}:/root/.ollama
    environment:
      - OLLAMA_HOST=http://0.0.0.0:11434
      - OLLAMA_MAX_LOADED_MODELS=0
      - CUDA_VISIBLE_DEVICES=""

  chromadb:
    image: chromadb/chroma:latest
    container_name: chromadb
    ports:
      - "${CHROMADB_PORT:-8000}:8000"
    volumes:
      - ${CHROMA_DATA_PATH:-../data/chroma_data}:/chroma/chroma
    environment:
      - ALLOW_RESET=true
      - ANONYMIZED_TELEMETRY=false
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_HTTP_PORT=8000
      - CHROMA_SERVER_CORS_ALLOW_ORIGINS=["*"]
    restart: unless-stopped

  tts:
    image: opea-docker-tts:latest
    container_name: tts
    ports:
      - "${TTS_SERVICE_PORT:-9200}:9200"
    volumes:
      - ${TTS_DATA_PATH:-../data/tts_data}:/app/data/tts_data
      - ${SHARED_DB_PATH:-../data/shared_db}:/app/db
    environment:
      - TTS_HOME=/app/data/tts_data
      - TTS_DATA_PATH=/app/data/tts_data
      - TTS_MODEL=${TTS_MODEL:-xtts_v2}
      - TTS_SERVICE_PORT=${TTS_SERVICE_PORT:-9200}
      - FORCE_CPU=true
      - CUDA_VISIBLE_DEVICES=""
    depends_on:
      - ollama-server
    restart: unless-stopped

  asr:
    image: opea-docker-asr:latest
    container_name: asr
    ports:
      - "${ASR_SERVICE_PORT:-9300}:9300"
    volumes:
      - ${ASR_DATA_PATH:-../data/asr_data}:/app/data/whisper
      - ${SHARED_DB_PATH:-../data/shared_db}:/app/db
    environment:
      - ASR_SERVICE_PORT=${ASR_SERVICE_PORT:-9300}
      - ASR_MODEL=${ASR_MODEL:-base}
      - FORCE_CPU=true
      - CUDA_VISIBLE_DEVICES=""
      - WHISPER_COMPUTE_TYPE=int8
    depends_on:
      - tts
    restart: unless-stopped

  llm-vision:
    image: opea-docker-llm-vision:latest
    container_name: llm-vision
    ports:
      - "${LLM_VISION_PORT:-9101}:9101"
    volumes:
      - ${SHARED_DB_PATH:-../data/shared_db}:/app/db
    environment:
      - VISION_SERVICE_PORT=${LLM_VISION_PORT:-9101}
      - VISION_MODEL_ID=${VISION_MODEL_ID:-llava-hf/llava-1.5-7b-hf}
      - CUDA_VISIBLE_DEVICES=""
    depends_on:
      - ollama-server
    restart: unless-stopped

  mangaocr:
    image: opea-docker-mangaocr:latest
    container_name: mangaocr
    ports:
      - "${MANGAOCR_PORT:-9100}:9100"
    volumes:
      - ${MANGAOCR_MODELS_PATH:-../data/mangaocr_models}:/app/data/mangaocr_models
      - ${SHARED_DB_PATH:-../data/shared_db}:/app/db
    environment:
      - MANGAOCR_MODEL_PATH=/app/data/mangaocr_models
      - MANGAOCR_SERVICE_PORT=${MANGAOCR_PORT:-9100}
      - FORCE_CPU=true
      - CUDA_VISIBLE_DEVICES=""
    depends_on:
      - ollama-server
    restart: unless-stopped

  waifu-diffusion:
    image: opea-docker-waifu-diffusion:latest
    container_name: waifu-diffusion
    ports:
      - "${WAIFU_DIFFUSION_PORT:-9500}:9500"
    volumes:
      - ${WAIFU_DATA_PATH:-../data/waifu}:/app/data/waifu-diffusion
      - ${SHARED_DB_PATH:-../data/shared_db}:/app/db
    environment:
      - WAIFU_DIFFUSION_PORT=${WAIFU_DIFFUSION_PORT:-9500}
      - WAIFU_MODEL_ID=${WAIFU_MODEL_ID:-hakurei/waifu-diffusion}
      - CUDA_VISIBLE_DEVICES=""
      - FORCE_CPU=true
    restart: unless-stopped

  guardrails:
    image: opea-docker-guardrails:latest
    container_name: guardrails
    ports:
      - "${GUARDRAILS_SERVICE_PORT:-9400}:9400"
    volumes:
      - ${OLLAMA_DATA_PATH:-../data/ollama_data}:/root/.ollama
      - ${SHARED_DB_PATH:-../data/shared_db}:/app/db
    environment:
      - OLLAMA_HOST=ollama-server
      - GUARDRAILS_SERVICE_PORT=${GUARDRAILS_SERVICE_PORT:-9400}
      - FORCE_CPU=true
      - CUDA_VISIBLE_DEVICES=""
    depends_on:
      - ollama-server
      - embeddings
    restart: unless-stopped

  embeddings:
    image: opea-docker-embeddings:latest
    container_name: embeddings
    ports:
      - "${EMBEDDING_SERVICE_PORT:-6000}:6000"
    volumes:
      - ${EMBEDDING_DATA_PATH:-../data/embeddings}:/app/data/embeddings
      - ${SHARED_DB_PATH:-../data/shared_db}:/app/db
    environment:
      - EMBEDDING_SERVICE_HOST_IP=0.0.0.0
      - EMBEDDING_SERVICE_PORT=${EMBEDDING_SERVICE_PORT:-6000}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-all-MiniLM-L6-v2}
      - CUDA_VISIBLE_DEVICES=""
    depends_on:
      - chromadb
    restart: unless-stopped

  # Visual Novel Services
  vn-game-server:
    build:
      context: ../visual-novel/server
    container_name: vn-game-server
    ports:
      - "8080:8080"
    volumes:
      - ../visual-novel/server:/app
      - ${SHARED_DB_PATH:-../data/shared_db}:/app/data/shared_db
    environment:
      - OLLAMA_SERVER_URL=http://ollama-server:11434
      - LLM_TEXT_URL=http://ollama-server:11434
      - GUARDRAILS_URL=http://guardrails:9400
      - CHROMADB_URL=http://chromadb:8000
      - TTS_URL=http://tts:9200
      - ASR_URL=http://asr:9300
      - LLM_VISION_URL=http://llm-vision:9101
      - WAIFU_DIFFUSION_URL=http://waifu-diffusion:9500
      - EMBEDDINGS_URL=http://embeddings:6000
      - USE_REMOTE_DB=true
      - OLLAMA_HOST=http://ollama-server:11434
      - OLLAMA_MODEL=llama3.2
      - DB_PATH=/app/data/shared_db/visual_novel.db
      - PORT=8080
      - FLASK_SERVER_URL=http://localhost:8080
    depends_on:
      - ollama-server
      - guardrails
      - chromadb
      - tts
      - asr
      - llm-vision
      - waifu-diffusion
      - embeddings
    restart: unless-stopped

  vn-web-server:
    image: nginx:alpine
    container_name: vn-web-server
    ports:
      - "8001:80"
    volumes:
      - ../visual-novel/web/visualnovel-1.0-web:/usr/share/nginx/html
      - ../visual-novel/nginx.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - vn-game-server
    restart: unless-stopped

volumes:
  ollama_data:

networks:
  default:
    name: opea-network
