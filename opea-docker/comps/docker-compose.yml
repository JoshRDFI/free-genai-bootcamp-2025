version: '3.8'

services:
  # Base Services (No Dependencies)
  ollama-server:
    image: ollama/ollama
    container_name: ollama-server
    ports:
      - "${LLM_ENDPOINT_PORT:-8008}:11434"
    volumes:
      - ${OLLAMA_DATA_PATH:-../../data/ollama_data}:/root/.ollama
      - ${SHARED_DB_PATH:-../../data/shared_db}:/app/db
    environment:
      - no_proxy=${no_proxy}
      - http_proxy=${http_proxy}
      - https_proxy=${https_proxy}
      - LLM_MODEL_ID=${LLM_MODEL_ID:-llama3.2}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  chromadb:
    image: ghcr.io/chroma-core/chroma:latest
    container_name: chromadb
    volumes:
      - ${CHROMA_DATA_PATH:-../../data/chroma_data}:/data
      - ${SHARED_DB_PATH:-../../data/shared_db}:/app/db
    ports:
      - "${CHROMADB_PORT:-8050}:8050"
    environment:
      - CHROMA_SERVER_PORT=${CHROMADB_PORT:-8050}
    restart: unless-stopped

  # Core Services (Depend on Base Services)
  llm_text:
    build:
      context: ./llm_text
      dockerfile: Dockerfile
    container_name: llm_text
    ports:
      - "${LLM_TEXT_PORT:-9000}:9000"
    environment:
      - LLM_ENDPOINT=${LLM_ENDPOINT}
      - LLM_SERVICE_PORT=${LLM_TEXT_PORT:-9000}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-llama3.2}
    depends_on:
      - ollama-server
    volumes:
      - ./llm_text:/app
      - ${SHARED_DB_PATH:-../../data/shared_db}:/app/db
      - ${LLM_XTTS_DATA_PATH:-../../data/llm_xtts}:/home/llm/.xtts_data
    restart: unless-stopped

  embeddings:
    build:
      context: ./embeddings
      dockerfile: Dockerfile
    container_name: embeddings
    ports:
      - "${EMBEDDING_SERVICE_PORT:-6000}:6000"
    environment:
      - EMBEDDING_SERVICE_HOST_IP=0.0.0.0
      - EMBEDDING_SERVICE_PORT=${EMBEDDING_SERVICE_PORT:-6000}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-all-MiniLM-L6-v2}
    depends_on:
      - chromadb
    volumes:
      - ./embeddings:/app
      - ${SHARED_DB_PATH:-../../data/shared_db}:/app/db
    restart: unless-stopped

  # AI Services (Depend on Core Services)
  llm-vision:
    build:
      context: ./llm-vision
      dockerfile: Dockerfile
    container_name: llm-vision
    ports:
      - "${LLM_VISION_PORT:-9100}:9100"
    environment:
      - VISION_MODEL_ID=${VISION_MODEL_ID:-llava:13b}
      - VISION_SERVICE_PORT=${LLM_VISION_PORT:-9100}
    depends_on:
      - llm_text
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ./llm-vision:/app
      - ${SHARED_DB_PATH:-../../data/shared_db}:/app/db
      - ${MANGAOCR_MODELS_PATH:-../../data/mangaocr_models}:/app/mangaocr_models
    restart: unless-stopped

  tts:
    build:
      context: ./tts
      dockerfile: Dockerfile
    container_name: tts
    ports:
      - "${TTS_SERVICE_PORT:-9200}:9200"
    environment:
      - TTS_SERVICE_PORT=${TTS_SERVICE_PORT:-9200}
      - TTS_MODEL=${TTS_MODEL:-tts_models/en/ljspeech/tacotron2-DDC}
    depends_on:
      - llm_text
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ./tts:/app
      - ${SHARED_DB_PATH:-../../data/shared_db}:/app/db
      - ${TTS_DATA_PATH:-../../data/tts_data}:/app/data/tts_data
    restart: unless-stopped

  asr:
    build:
      context: ./asr
      dockerfile: Dockerfile
    container_name: asr
    ports:
      - "${ASR_SERVICE_PORT:-9300}:9300"
    environment:
      - ASR_SERVICE_PORT=${ASR_SERVICE_PORT:-9300}
      - ASR_MODEL=${ASR_MODEL:-facebook/wav2vec2-base-960h}
    depends_on:
      - tts
    volumes:
      - ./asr:/app
      - ${SHARED_DB_PATH:-../../data/shared_db}:/app/db
      - ${ASR_DATA_PATH:-../../data/asr_data}:/app/data/asr_data
    restart: unless-stopped

  # Utility Services
  guardrails:
    build:
      context: ./guardrails
      dockerfile: Dockerfile
    container_name: guardrails
    ports:
      - "${GUARDRAILS_SERVICE_PORT:-9400}:9400"
    environment:
      - GUARDRAILS_SERVICE_PORT=${GUARDRAILS_SERVICE_PORT:-9400}
    depends_on:
      - llm_text
      - embeddings
    volumes:
      - ./guardrails:/app
      - ${SHARED_DB_PATH:-../../data/shared_db}:/app/db
    restart: unless-stopped

  waifu-diffusion:
    build:
      context: ./waifu-diffusion
      dockerfile: Dockerfile
    container_name: waifu-diffusion
    ports:
      - "${WAIFU_DIFFUSION_PORT:-9500}:9500"
    environment:
      - WAIFU_DIFFUSION_PORT=${WAIFU_DIFFUSION_PORT:-9500}
      - WAIFU_MODEL_ID=${WAIFU_MODEL_ID:-waifu-diffusion/wd-1-5-beta2}
    depends_on:
      - llm_text
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ./waifu-diffusion:/app
      - ${SHARED_DB_PATH:-../../data/shared_db}:/app/db
      - ${WAIFU_DATA_PATH:-../../data/waifu}:/app/data/waifu
    restart: unless-stopped

  vocabulary_generator:
    build:
      context: ./vocabulary_generator
      dockerfile: Dockerfile
    container_name: vocabulary_generator
    ports:
      - "9103:9103"
    depends_on:
      - llm_text
      - embeddings
    volumes:
      - ./vocabulary_generator:/app
      - ${SHARED_DB_PATH:-../../data/shared_db}:/app/data/shared_db
      - ${SHARED_DB_PATH:-../../data/shared_db}:/app/database
      - ../../data/vocabulary_generator:/app/data/vocabulary_generator
    environment:
      - PYTHONUNBUFFERED=1
      - DB_PATH=/app/data/shared_db/db.sqlite3
      - SCHEMA_PATH=/app/data/shared_db/schema.sql
    restart: unless-stopped