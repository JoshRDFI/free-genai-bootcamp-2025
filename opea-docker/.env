# Main environment variables for all opea-docker services

COMPOSE_BAKE=true

# Ollama LLM Service Configuration
LLM_ENDPOINT_PORT=8008
LLM_ENDPOINT=http://ollama-server:11434
LLM_MODEL_ID=llama3.2

# Proxy settings
no_proxy=localhost,127.0.0.1
http_proxy=
https_proxy=

# Host IP configuration
host_ip=127.0.0.1

# Embedding Service Configuration
EMBEDDING_SERVICE_HOST_IP=0.0.0.0
EMBEDDING_SERVICE_PORT=6000
EMBEDDING_MODEL=all-MiniLM-L6-v2

# LLM Vision Service Configuration
LLM_VISION_PORT=9100
VISION_MODEL_ID=llava

# TTS (Text-to-Speech) Service Configuration
TTS_SERVICE_PORT=9200
TTS_MODEL=xtts-v2

# ASR (Automatic Speech Recognition) Service Configuration
ASR_SERVICE_PORT=9300
ASR_MODEL=whisper-large-v3

# ChromaDB Configuration
CHROMADB_PORT=8050

# Guardrails Service Configuration
GUARDRAILS_SERVICE_PORT=9400

# Shared data paths
DATA_DIR=./data
SHARED_DB_PATH=./data/shared_db
OLLAMA_DATA_PATH=./data/ollama_data
CHROMA_DATA_PATH=./data/chroma_data
TTS_DATA_PATH=./data/tts_data
ASR_DATA_PATH=./data/asr_data
LLM_XTTS_DATA_PATH=./data/llm_xtts
MANGAOCR_MODELS_PATH=./data/mangaocr_models

# Resource allocation
GPU_ENABLED=true